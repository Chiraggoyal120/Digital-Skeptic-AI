# -*- coding: utf-8 -*-
"""Digital_skeptic_AI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0dX7XlN1SarpSS87y4EEa0QxiEy_kPO
"""

# Essential packages
!pip install requests beautifulsoup4 gradio

# Additional HTML parsers (recommended for better content extraction)
!pip install lxml html5lib

# Optional: For better text processing
!pip install urllib3

"""
Digital Skeptic AI - Hackathon Mission 2
A multi-agent system for critical analysis of news articles using OpenRouter API
Author: Hackathon Participant
Description: Empowers critical thinking by providing comprehensive analysis of online news articles
"""

import requests
from bs4 import BeautifulSoup
import gradio as gr
import json
import re
from datetime import datetime
from urllib.parse import urlparse
import time
from typing import Dict, List, Tuple, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DigitalSkepticAI:
    """
    A multi-agent system for critical analysis of news articles.
    Uses specialized agents for different aspects of analysis to ensure comprehensive coverage.
    """

    def __init__(self, api_key: str, model: str = "anthropic/claude-3-haiku"):
        """
        Initialize the Digital Skeptic AI system.

        Args:
            api_key (str): OpenRouter API key
            model (str): Model to use for analysis
        """
        self.api_key = api_key
        self.model = model
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://colab.research.google.com",
            "X-Title": "Digital Skeptic AI"
        }

    def fetch_article_content(self, url: str) -> Tuple[str, str, str]:
        """
        Fetch and extract article content from URL with robust error handling.

        Args:
            url (str): URL of the news article

        Returns:
            Tuple[str, str, str]: (title, content, error_message)
        """
        try:
            # Add user agent to avoid blocking
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract title
            title = soup.find('title')
            title = title.get_text().strip() if title else "Unknown Title"

            # Remove script and style elements
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()

            # Try multiple selectors for article content
            content_selectors = [
                'article', '[role="main"]', '.article-content', '.post-content',
                '.entry-content', '.content', 'main', '.article-body'
            ]

            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text() for elem in elements])
                    break

            # Fallback: get all paragraph text
            if not content:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text() for p in paragraphs])

            # Clean up the content
            content = re.sub(r'\s+', ' ', content).strip()

            if len(content) < 100:
                return title, "", "Article content appears too short or could not be extracted properly."

            return title, content, ""

        except requests.exceptions.RequestException as e:
            return "", "", f"Error fetching URL: {str(e)}"
        except Exception as e:
            return "", "", f"Error parsing content: {str(e)}"

    def call_openrouter_api(self, messages: List[Dict], temperature: float = 0.3) -> Optional[str]:
        """
        Make API call to OpenRouter with error handling and retries.

        Args:
            messages (List[Dict]): Messages for the API call
            temperature (float): Temperature parameter for generation

        Returns:
            Optional[str]: Response content or None if failed
        """
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": 2000
            }

            response = requests.post(self.base_url, headers=self.headers, json=payload)
            response.raise_for_status()

            result = response.json()
            return result['choices'][0]['message']['content']

        except Exception as e:
            logger.error(f"API call failed: {str(e)}")
            return None

    def extract_core_claims_agent(self, title: str, content: str) -> str:
        """
        Specialized agent for extracting core factual claims from the article.
        """
        prompt = f"""You are a specialized fact-extraction agent. Your job is to identify the 3-5 most important FACTUAL CLAIMS made in this news article. Focus on concrete, verifiable statements rather than opinions.

Article Title: {title}
Article Content: {content[:3000]}...

Instructions:
1. Identify 3-5 core factual claims (not opinions or speculation)
2. Each claim should be specific and verifiable
3. Focus on the most newsworthy and significant assertions
4. Present as bullet points starting with "* "
5. Be concise but specific

Format your response as:
* [Specific factual claim 1]
* [Specific factual claim 2]
* [Specific factual claim 3]
* [Additional claims if significant]

Core Claims:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages)
        return response or "* Could not extract core claims due to API error"

    def language_analysis_agent(self, title: str, content: str) -> str:
        """
        Specialized agent for analyzing language, tone, and writing style.
        """
        prompt = f"""You are a linguistics and rhetoric analysis expert. Analyze the language, tone, and writing style of this news article.

Article Title: {title}
Article Content: {content[:2500]}...

Analyze:
1. Overall tone (neutral, emotional, persuasive, sensational, etc.)
2. Language choices (formal, casual, technical, loaded terms)
3. Writing style (objective reporting, opinion, advocacy, etc.)
4. Emotional appeals or rhetorical devices used
5. Level of certainty in claims (definitive vs. cautious language)

Provide a 2-3 sentence analysis focusing on how the language choices might influence reader perception.

Language & Tone Analysis:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages)
        return response or "Could not analyze language due to API error"

    def bias_detection_agent(self, title: str, content: str) -> str:
        """
        Specialized agent for detecting potential bias and red flags in reporting.
        """
        prompt = f"""You are a media literacy expert specializing in bias detection and journalistic standards. Identify potential red flags in this article.

Article Title: {title}
Article Content: {content[:2500]}...

Look for these red flags:
1. Over-reliance on anonymous sources
2. Lack of supporting evidence or data
3. Missing opposing viewpoints
4. Loaded or emotionally manipulative language
5. Logical fallacies or weak reasoning
6. Selective fact presentation
7. Sensationalized headlines vs. content
8. Lack of expert sources or credentials
9. Unsubstantiated generalizations
10. Cherry-picked statistics or quotes

Format as bullet points starting with "* ". Only include red flags you actually detect. If the article appears well-balanced, note that too.

Potential Red Flags:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages)
        return response or "* Could not analyze for bias due to API error"

    def verification_questions_agent(self, title: str, content: str, claims: str) -> str:
        """
        Specialized agent for generating specific verification questions.
        """
        prompt = f"""You are a fact-checking expert. Based on this article and its core claims, generate 3-4 specific, actionable questions that readers should ask to independently verify the information.

Article Title: {title}
Core Claims: {claims}
Article Content: {content[:2000]}...

Generate questions that:
1. Are specific and actionable (not vague)
2. Help verify the most important claims
3. Encourage checking multiple sources
4. Look into potential conflicts of interest
5. Examine the credibility of sources cited

Format as numbered list:
1. [Specific verification question]
2. [Specific verification question]
3. [Specific verification question]
4. [Additional question if needed]

Verification Questions:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages)
        return response or "1. Could not generate verification questions due to API error"

    def entity_recognition_agent(self, content: str) -> str:
        """
        Advanced agent for identifying key entities and suggesting investigation points.
        """
        prompt = f"""You are an investigative research specialist. Identify key people, organizations, and entities mentioned in this article and suggest what readers should investigate about them.

Article Content: {content[:2000]}...

For each significant entity (people, organizations, companies, institutions):
1. Identify their role in the story
2. Suggest specific investigation points (funding, bias, track record, etc.)

Format as:
**[Entity Name]**: [Role in story] - *Investigate: [specific research suggestions]*

Focus on the most important 3-5 entities that would benefit from reader investigation.

Entity Investigation Guide:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages, temperature=0.4)
        return response or "Could not identify key entities due to API error"

    def counter_argument_agent(self, title: str, content: str) -> str:
        """
        Agent that simulates opposing viewpoints to highlight potential bias.
        """
        prompt = f"""You are playing devil's advocate. Based on this article, present how someone with an opposing viewpoint might interpret or critique the same information.

Article Title: {title}
Article Content: {content[:2000]}...

Instructions:
1. Don't make up facts, but reinterpret existing information
2. Highlight what the article might be overlooking
3. Present alternative explanations for the events described
4. Point out potential selection bias in sources or examples
5. Keep it balanced and constructive

Provide a 2-3 sentence summary of how an opposing viewpoint might frame this story.

Alternative Perspective:"""

        messages = [{"role": "user", "content": prompt}]
        response = self.call_openrouter_api(messages, temperature=0.5)
        return response or "Could not generate alternative perspective due to API error"

    def generate_report(self, url: str) -> str:
        """
        Orchestrate the multi-agent analysis to generate comprehensive report.

        Args:
            url (str): URL of the news article to analyze

        Returns:
            str: Complete markdown report
        """
        # Validate URL
        try:
            parsed = urlparse(url)
            if not parsed.netloc:
                return "‚ùå **Error**: Please provide a valid URL (include http:// or https://)"
        except:
            return "‚ùå **Error**: Invalid URL format"

        # Step 1: Fetch article content
        title, content, error = self.fetch_article_content(url)
        if error:
            return f"‚ùå **Error fetching article**: {error}\n\nüí° **Tip**: Try saving the article text to a file if the website blocks automated access."

        if not content:
            return "‚ùå **Error**: Could not extract article content. The website may be blocking automated access."

        # Step 2: Run multi-agent analysis in parallel (simulated)
        logger.info("Starting multi-agent analysis...")

        # Core analysis agents
        claims = self.extract_core_claims_agent(title, content)
        time.sleep(0.5)  # Rate limiting

        language_analysis = self.language_analysis_agent(title, content)
        time.sleep(0.5)

        red_flags = self.bias_detection_agent(title, content)
        time.sleep(0.5)

        verification_questions = self.verification_questions_agent(title, content, claims)
        time.sleep(0.5)

        # Advanced analysis agents
        entity_analysis = self.entity_recognition_agent(content)
        time.sleep(0.5)

        counter_perspective = self.counter_argument_agent(title, content)

        # Step 3: Compile comprehensive report
        report = f"""# üîç Critical Analysis Report

**Article**: [{title}]({url})
**Analysis Date**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Word Count**: {len(content.split())} words

---

## üìã Core Claims
{claims}

---

## üéØ Language & Tone Analysis
{language_analysis}

---

## ‚ö†Ô∏è Potential Red Flags
{red_flags}

---

## ‚ùì Verification Questions
{verification_questions}

---

## üîç Key Entities to Investigate
{entity_analysis}

---

## üîÑ Alternative Perspective
{counter_perspective}

---

## üìä Analysis Summary
- **Content Length**: {len(content.split())} words
- **Analysis Depth**: Multi-agent comprehensive review
- **Bias Risk Level**: {'High' if 'anonymous' in red_flags.lower() or 'loaded' in red_flags.lower() else 'Medium' if 'red flag' in red_flags.lower() else 'Low'}

---

*Generated by Digital Skeptic AI - Empowering Critical Thinking*
"""

        logger.info("Analysis complete!")
        return report

def create_gradio_interface():
    """
    Create and configure the Gradio interface for the Digital Skeptic AI.
    """

    # Global variable to store the AI instance
    skeptic_ai = None

    def setup_ai(api_key: str, model: str = "anthropic/claude-3-haiku") -> str:
        """Setup the AI instance with provided API key."""
        nonlocal skeptic_ai
        if not api_key.strip():
            return "‚ö†Ô∏è Please enter your OpenRouter API key"

        skeptic_ai = DigitalSkepticAI(api_key.strip(), model)
        return f"‚úÖ Digital Skeptic AI initialized with model: {model}"

    def analyze_article(url: str) -> str:
        """Analyze article using the initialized AI instance."""
        if not skeptic_ai:
            return "‚ö†Ô∏è Please set up your API key first using the Setup tab"

        if not url.strip():
            return "‚ö†Ô∏è Please enter a valid article URL"

        return skeptic_ai.generate_report(url.strip())

    # Create the interface
    with gr.Blocks(
        title="Digital Skeptic AI - Critical News Analysis",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .header {
            text-align: center;
            padding: 20px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        """
    ) as interface:

        # Header
        gr.HTML("""
        <div class="header">
            <h1>üîç Digital Skeptic AI</h1>
            <p><i>Empowering Critical Thinking in an Age of Information Overload</i></p>
        </div>
        """)

        with gr.Tabs():
            # Main Analysis Tab
            with gr.Tab("üì∞ Article Analysis"):
                gr.Markdown("""
                ### How to Use:
                1. First, set up your OpenRouter API key in the **Setup** tab
                2. Enter the URL of any news article you want to analyze
                3. Click **Analyze Article** to get a comprehensive critical analysis

                The AI will provide core claims, language analysis, potential red flags, verification questions, and more!
                """)

                url_input = gr.Textbox(
                    label="Article URL",
                    placeholder="https://example.com/news-article",
                    lines=1
                )

                analyze_btn = gr.Button("üîç Analyze Article", variant="primary", size="lg")

                output = gr.Markdown(
                    label="Analysis Report",
                    value="Enter an article URL above and click 'Analyze Article' to begin...",
                    elem_classes=["output-markdown"]
                )

                analyze_btn.click(
                    fn=analyze_article,
                    inputs=[url_input],
                    outputs=[output],
                    show_progress=True
                )

            # Setup Tab
            with gr.Tab("‚öôÔ∏è Setup"):
                gr.Markdown("""
                ### Setup Instructions:
                1. Get your API key from [OpenRouter.ai](https://openrouter.ai/keys)
                2. Enter it below and click **Initialize AI**
                3. Choose your preferred model (Claude Haiku is fast and cost-effective)
                """)

                api_key_input = gr.Textbox(
                    label="OpenRouter API Key",
                    type="password",
                    placeholder="sk-or-v1-..."
                )

                model_dropdown = gr.Dropdown(
                    label="AI Model",
                    choices=[
                        "anthropic/claude-3-haiku",
                        "anthropic/claude-3-sonnet",
                        "openai/gpt-4o-mini",
                        "openai/gpt-4o"
                    ],
                    value="anthropic/claude-3-haiku"
                )

                setup_btn = gr.Button("üöÄ Initialize AI", variant="primary")
                setup_status = gr.Textbox(label="Status", interactive=False)

                setup_btn.click(
                    fn=setup_ai,
                    inputs=[api_key_input, model_dropdown],
                    outputs=[setup_status]
                )

            # About Tab
            with gr.Tab("‚ÑπÔ∏è About"):
                gr.Markdown("""
                ## Digital Skeptic AI

                **Mission**: Empower critical thinking by providing comprehensive analysis of online news articles.

                ### Features:
                - **Multi-Agent Analysis**: Specialized AI agents for different aspects of critical analysis
                - **Core Claims Extraction**: Identifies main factual assertions
                - **Language & Tone Analysis**: Examines rhetoric and writing style
                - **Bias Detection**: Flags potential red flags in reporting
                - **Verification Questions**: Provides specific questions for fact-checking
                - **Entity Investigation**: Identifies key players and suggests research points
                - **Alternative Perspectives**: Shows how opposing viewpoints might interpret the story

                ### Technical Architecture:
                - **Web Scraping**: Robust content extraction with fallback mechanisms
                - **API Integration**: OpenRouter for accessing multiple AI models
                - **Error Handling**: Comprehensive error management and user feedback
                - **Rate Limiting**: Responsible API usage

                ### Models Supported:
                - Anthropic Claude (Haiku, Sonnet)
                - OpenAI GPT-4o (Mini, Full)

                ---
                *Built for the "Freedom from Mundane: AI for a Smarter Life" Hackathon*
                """)

    return interface

# Main execution
if __name__ == "__main__":
    # Create and launch the interface
    interface = create_gradio_interface()

    print("üöÄ Starting Digital Skeptic AI...")
    print("üìñ Instructions:")
    print("1. Go to the Setup tab and enter your OpenRouter API key")
    print("2. Use the Article Analysis tab to analyze news articles")
    print("3. Get comprehensive critical analysis reports!")

    # Launch with public sharing for Colab
    interface.launch(
        share=True,  # Creates public URL for Colab
        server_name="0.0.0.0",  # Accept connections from any IP
        server_port=7860,  # Default Gradio port
        show_error=True,  # Show detailed errors
        quiet=False  # Show startup logs
    )